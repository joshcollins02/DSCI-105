{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1d75b3-e7d5-469f-9844-1a699061f757",
   "metadata": {},
   "source": [
    "# Homework 8: Web Scraping and Database Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97730245-44be-42ab-aaca-55e0dac8da97",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is an **pair-optional** assignment. Total: 20 points. Due:**<span style=\"color:red\">  Sunday, November 5, 10:00 pm </span>**.\n",
    "\n",
    "**Overview**\n",
    "In this assignment, you will develop a Python script to scrape book information from the website https://books.toscrape.com. The website contains a total of 1000 book entries spread across multiple pages. Your task is to extract information about each book, including its `title`,`category`, `price`, `availability`, and `description`, and then store this information in a MySQL database. After populating the database, you are required to perform **at least five** meaningful MySQL queries to demonstrate your data manipulation skills.\n",
    "\n",
    "**Objectives**\n",
    "1. **Web Scraping**: Write a Python script using libraries like `requests` and `BeautifulSoup` to scrape book data from the website. Ensure you navigate through all the pages to get details of all 1000 books.\n",
    "2. **Database Creation**: Set up a MySQL database and design an appropriate schema to store the scraped book data.\n",
    "3. **Data Insertion**: Populate the MySQL database with the scraped data, ensuring data integrity and proper structuring.\n",
    "4. **MySQL Queries**: Execute **at least five** meaningful queries on your database. These might include:\n",
    "    * Aggregations (e.g., average price of books, count of books under specific categories).\n",
    "    * Search queries (e.g., finding all books with a particular word in their title/description).\n",
    "    * Data updates (e.g., updating prices or availability status).\n",
    "    * Ordering and grouping of data based on certain criteria.\n",
    "\n",
    "**Deliverables**\n",
    "* Python script for Web Scraping: A script that systematically navigates through https://books.toscrape.com and scrapes the relevant data.\n",
    "* The Database and the Python script used to populate it: Exporting a database will be demonstrated on Friday.\n",
    "* MySQL script: MySQL queries executed on the database, along with brief explanations of the purpose and outcome of each query.\n",
    "* Report (Optional) - A concise report including:\n",
    "    * Challenges encountered and how they were resolved.\n",
    "    * Insights or interesting findings from your MySQL queries.\n",
    "    * Any suggestions or advice for this assignment.\n",
    "    \n",
    "\n",
    "**Submission**\n",
    "* Include the Python scripts and MySQL queries in the Jupyter Notebook. Submit the actual database as a .zip or .sql file on Gradescope, along with the Jupyter Notebook.\n",
    "\n",
    "**Notes**\n",
    "* Your code should handle exceptions and potential data inconsistencies gracefully.\n",
    "* Pay attention to the pagination on the website when designing your scraper.\n",
    "* You may need to visit the specific page of the book to obtain its category and description. It's possible that certain books may lack a description.\n",
    "* Based on your approach, the code snippet `find_all('p', recursive=False)` could be helpful for extracting the description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d79ad-16c1-47da-ab96-df8d9acc564e",
   "metadata": {},
   "source": [
    "# For pagination use a for loop:\n",
    "\n",
    "for page in range(1, 51):\n",
    " page_url = f'https://books.toscrape.com/catalogue/page-{page}.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cec9225-ac45-453f-880c-a5ca7547dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Obtaining dependency information for mysql-connector-python from https://files.pythonhosted.org/packages/a7/84/b63f11124f808b6f1e3389072bc36cc907929d7574e85f94bf8f18117fe4/mysql_connector_python-8.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading mysql_connector_python-8.2.0-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting protobuf<=4.21.12,>=4.21.1 (from mysql-connector-python)\n",
      "  Downloading protobuf-4.21.12-cp310-abi3-win_amd64.whl (527 kB)\n",
      "     ---------------------------------------- 0.0/527.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/527.0 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/527.0 kB ? eta -:--:--\n",
      "     -- ---------------------------------- 41.0/527.0 kB 393.8 kB/s eta 0:00:02\n",
      "     ---------------- --------------------- 235.5/527.0 kB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 527.0/527.0 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading mysql_connector_python-8.2.0-cp311-cp311-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.2 MB 25.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.4/14.2 MB 14.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.2/14.2 MB 15.7 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.0/14.2 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.8/14.2 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.5/14.2 MB 15.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.3/14.2 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.9/14.2 MB 15.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.8/14.2 MB 16.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.6/14.2 MB 16.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.4/14.2 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.0/14.2 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.9/14.2 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.8/14.2 MB 16.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.9/14.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.6/14.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.4/14.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 15.9 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.2.0 protobuf-4.21.12\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mysql-connector-python\n",
    "# Had to reinstall cuz my computer broke have way through and got a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b89acd-910e-4f50-9928-32a1627da63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Trying to mix the two scripts because I cant figure out how to transfer data\n",
    "import mysql.connector\n",
    "import random\n",
    "from mysql.connector import Error\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://books.toscrape.com/catalogue/\"\n",
    "# Sending a request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "#func\n",
    "def web_scrape(booki_url):\n",
    "    response = requests.get(booki_url)\n",
    "    data = response.text\n",
    "# Parsing the HTML content of the webpage\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "# Selecting all elements that contain the book information\n",
    "    book_containers = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    book_title = soup.find('h1').get_text(strip=True)\n",
    "#select nested text son! (second element of list)\n",
    "    book_genre = soup.select(\"ul.breadcrumb li\")[2].get_text(strip=True)\n",
    "#select_one first instance of element\n",
    "    book_price = soup.select_one('p.price_color').get_text(strip=True)\n",
    "    book_availability = soup.find('p', class_='availability').get_text(strip=True)\n",
    "#meta tag cuz I couldn't figure out recursive statement\n",
    "    book_description = soup.find(\"meta\", attrs={\"name\": \"description\"})[\"content\"]    \n",
    "    \n",
    "    return book_title, book_genre, book_price, book_availability, book_description\n",
    "\n",
    "try:\n",
    "# Establish a database connection\n",
    "    connection = mysql.connector.connect(user='root', password='rootpassword', host='localhost', database='HW8')\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "# SQL query for inserting data\n",
    "    add_transaction = (\"INSERT INTO scrapey \"\n",
    "                       \"(title, genre, price, availability, description) \"\n",
    "                       \"VALUES (%s, %s, %s, %s, %s)\")\n",
    "\n",
    "# Pagination dawg\n",
    "    for page in range(1, 51): \n",
    "        print(page)\n",
    "        page_url = f'https://books.toscrape.com/catalogue/page-{page}.html'    \n",
    "        response = requests.get(page_url)\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        book_containers = soup.find_all('h3')\n",
    "# Iterate through each book container and extract info from book's individual url  \n",
    "        for book in book_containers:\n",
    "            close_url = book.find('a')['href']\n",
    "            booki_url = url + close_url\n",
    "            book_title, book_genre, book_price, book_availability, book_description = web_scrape(booki_url)\n",
    "\n",
    "            transaction_data = (book_title, book_genre, book_price, book_availability, book_description)\n",
    "# Insert new book\n",
    "            cursor.execute(add_transaction, transaction_data)\n",
    "# Commit the transaction\n",
    "    connection.commit()\n",
    "\n",
    "except mysql.connector.Error as error:\n",
    "    print(f\"Failed to insert record into MySQL table {error}\")\n",
    "\n",
    "finally:\n",
    "# Close communication with the database\n",
    "    if connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3af4af-8a4c-4e0b-ab0f-329d427f6584",
   "metadata": {},
   "source": [
    "### REPORT ###\n",
    "## Challenges encountered and how they were resolved.\n",
    "The biggest challenge that I faced was getting the web scraping to work. I tried so many variations in my code when it came to finding the genre, price and description. I ended up finding 'select' and used it instead of find because I couldn't get find to work for all variables.\n",
    "\n",
    "## Insights or interesting findings from your MySQL queries.\n",
    "Only 3 books had descriptions that were over 5,000 characters long, most fell into the range of 1,000-2,000 characters. \n",
    "No book cost more than 60 euros, so Im glad that they took time to give realistic prices instead of just randomly assigning prices without a limit.\n",
    "\n",
    "## Any suggestions or advice for this assignment.\n",
    "The assignment was extremely vague, in some ways I enoyed it and others I hated it. With the other assignments we had the modules to help with our answers, but this one I really only had youtube and reddit to help me (with the exception of the template and deep diving into the links you gave us). Maybe give a little bit more info on web scraping and how it works over a second class in the future because I was lost so many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa18786-dd66-4102-bcf5-e3b58a8f1505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
